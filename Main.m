env = MoonLander;
actionInfo = getActionInfo(env);
observationInfo = getObservationInfo(env);
numObs = observationInfo.Dimension(1);
numAct = numel(actionInfo.Elements);
dt = 0.1;
rng(0)

criticLayerSizes = [400 300];
policyLayerSizes = [400 300];

criticNetwork = [
        imageInputLayer([numObs 1 1],'Normalization','none','Name','observation')
        fullyConnectedLayer(criticLayerSizes(1), 'Name', 'CriticFC1', ...
            'Weights',sqrt(2/numObs)*(rand(criticLayerSizes(1),numObs)-0.5), ...
            'Bias',1e-3*ones(criticLayerSizes(1),1))
        reluLayer('Name', 'CriticRelu1')
        fullyConnectedLayer(criticLayerSizes(2), 'Name', 'CriticFC2', ...
            'Weights',sqrt(2/criticLayerSizes(1))*(rand(criticLayerSizes(2),criticLayerSizes(1))-0.5), ...
            'Bias',1e-3*ones(criticLayerSizes(2),1))
        reluLayer('Name', 'CriticRelu2')
        fullyConnectedLayer(1, 'Name', 'CriticOutput', ...
            'Weights',sqrt(2/criticLayerSizes(2))*(rand(1,criticLayerSizes(2))-0.5), ...
            'Bias',1e-3)];

criticOpts = rlRepresentationOptions('LearnRate',1e-4);
critic = rlValueRepresentation(criticNetwork, observationInfo, 'Observation', {'observation'}, criticOpts);



policyNetwork = [imageInputLayer([numObs 1 1],'Normalization','none','Name','observation')
        fullyConnectedLayer(policyLayerSizes(1),'Name','PolicyFC1', ...
            'Weights',sqrt(2/numObs)*(rand(policyLayerSizes(1),numObs)-0.5), ...
            'Bias',1e-3*ones(policyLayerSizes(1),1))
        reluLayer('Name', 'PolicyRelu1')
        fullyConnectedLayer(policyLayerSizes(2),'Name','PolicyFC2', ...
            'Weights',sqrt(2/policyLayerSizes(1))*(rand(policyLayerSizes(2),policyLayerSizes(1))-0.5), ...
            'Bias',1e-3*ones(policyLayerSizes(2),1))
        reluLayer('Name', 'PolicyRelu2')
        fullyConnectedLayer(numAct,'Name','Action', ...
            'Weights',sqrt(2/policyLayerSizes(2))*(rand(numAct,policyLayerSizes(2))-0.5), ...
            'Bias',1e-3*ones(numAct,1))
        softmaxLayer('Name','actionProb')];


policyOpts = rlRepresentationOptions('LearnRate',1e-4);
policy = rlStochasticActorRepresentation(policyNetwork, observationInfo, actionInfo, 'Observation', {'observation'}, policyOpts);


agentOpts = rlPPOAgentOptions(...
                'ExperienceHorizon',600,... 
                'ClipFactor',0.02,...
                'EntropyLossWeight',0.01,...
                'MiniBatchSize',128,...
                'NumEpoch',3,...
                'AdvantageEstimateMethod','gae',...
                'GAEFactor',0.95,...
                'SampleTime',dt,...
                'DiscountFactor',0.995);


agent = rlPPOAgent(policy, critic, agentOpts);





trainOpts = rlTrainingOptions(...
    'MaxEpisodes', 20000,...
    'MaxStepsPerEpisode',600,...
    'Plots','training-progress',...
    'StopTrainingCriteria','AverageReward',...
    'StopTrainingValue',400,...
    'ScoreAveragingWindowLength',100,...
    'SaveAgentCriteria',"EpisodeReward",...
    'SaveAgentValue',1000);



trainAgent = true;
if trainAgent   
    trainingStats = train(agent,env,trainOpts);
else
    load('Agent16200.mat');
end


plot(env)

simOptions = rlSimulationOptions('MaxSteps',600);
simOptions.NumSimulations = 10; 
experience = sim(env, agent, simOptions);



